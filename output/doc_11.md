by fine-tuning the model exclusively on a large-scale, high-quality Long-Chain-of-Thought (LongCoT) dataset.

LongCoT Data Curation. We develop a holistic data pipeline to build our high-quality LongCoT dataset. We first collect diverse datasets, including VisualWebInstruct (Jia et al., 2025), MathV360K (Shi et al., 2024a), and LLaVA-CoT (Xu et al., 2024a), and then apply a cleaning protocol which includes: (i) removing content extraneous to the reasoning process, such as system prompts and conflicting hints; (ii) unifying the output format by placing the thinking process within <think> and </think> tags, and the final answer in the \boxed{} tags; and (iii) deduplicating samples based on image and question pairs. For CoT data generation, we employ a guided prompting strategy where we provide both the question and its ground-truth, tasking the model to generate a detailed reasoning process that logically connects the two. Following this generation, we execute a rigorous filtering workflow. This workflow includes a final deduplication pass, format validation, and three critical quality filters: (i) Redundancy Filtering: We measure the token overlap between the generated CoT and the final answer, discarding samples with high similarity to penalize trivial reasoning. (ii) Answer Distillation: We utilize a judge model to refine and shorten overly verbose ground-truth answers, mitigating the tendency for models to overthink. (iii) CoT Length Balancing: We analyze the token length distribution of the generated CoT and resample to ensure a balanced representation of reasoning complexities. We finally get 400K LongCoT samples.

Training Recipe. We fine-tune the model on the 400K LongCoT samples. All model parameters are trained for one epoch using the AdamW optimizer with a global batch size of 1024. We employ a cosine learning rate schedule with a peak learning rate of  $ 1 \times 10^{-6} $ . The optimization objective is a standard next-token prediction over the entire output sequence:

 $$ \mathcal{L}_{\mathrm{LongCoT SFT}}=-\frac{1}{\left|D_{\mathrm{CoT}}\right|}\sum_{(I,T,A)\in D_{\mathrm{CoT}}}\log P_{\theta}(T\circ A|I) $$ 

where  $ D_{CoT} $  is the LongCoT dataset, I is the input instruction, T is the thinking process, A is the final answer, and  $ \circ $  denotes string concatenation. This training strategy explicitly teaches the model to generate a detailed reasoning process before providing the final answer, thereby significantly enhancing its ability to solve complex problems that require step-by-step thinking.

#### 4.2.3 Reinforcement Learning with Verifiable Rewards

Following the LongCoT Supervised Fine-tuning stage, we further enhance the model's reasoning capabilities through a Reasoning Reinforcement Learning (RL) stage. This stage refines the model by optimizing it against a reward system focused on two primary objectives: the correctness of the final answer and adherence to the specified output format.

Data Curation. The dataset for our RL stage is constructed from a diverse collection of public sources, covering tasks such as Math (Sun et al., 2024; Meng et al., 2025), Puzzle (Chia et al., 2024), Science (Wang et al., 2025b; Lu et al., 2022), OCR (Chen et al., 2025), and Counting (Johnson et al., 2017). This raw data is then refined through a two-stage filtering pipeline. The first stage mitigates reward hacking by employing an LLM to convert multiple-choice questions with numerical or symbolic answers into a free-response format. The second stage applies a difficulty-based filtering strategy using the model from the previous stage, evaluated on its pass@4 score. Specifically, for tasks that do not require explicit reasoning, we discard trivial problems where the model achieves a perfect score (pass@4=1). For reasoning-heavy tasks, we apply a more stringent filter, retaining problems that are challenging yet solvable by removing both the easiest (pass@4=1) and hardest (pass@4=0) samples. This pipeline yields a dataset of challenging yet tractable problems for stable RL training. We finally got 70K stem samples for our RL training.

Reward System. Our reward system is designed to guide the model towards generating accurate and well-structured responses. It is composed of two main signals: (1) Answer Reward: This is a rule-based reward that checks for the correctness of the final answer contained within the  $ \backslash $ boxed{} tag. For problems with verifiable solutions, such as mathematical calculations, it provides a clear, objective signal of success; (2) Format Reward: This reward ensures the model's output adheres to the required structure, specifically by verifying that the reasoning process is correctly enclosed within  $ \langle $ think $ \rangle $  and  $ \langle/ $ think $ \rangle $  tags. Both reward signals are discrete binary values (0 or 1), providing direct and unambiguous feedback for RL training.

Training Recipe. We optimize the LongCoT SFT model using a Proximal Policy Optimization (PPO)-based (Schulman et al., 2017) algorithm to learn a single, unified reasoning policy. To accommodate different model architectures, we employ specialized PPO variants. For our dense model, we use DAPO (Yu et al., 2025), a memory-efficient optimizer that enhances training stability. For our Mixture-of-