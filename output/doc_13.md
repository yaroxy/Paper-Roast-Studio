Reward System. Our mixed reward system is comprised of three distinct signals, designed to guide the model towards robust and trustworthy reasoning. The first is an Answer Reward that scores the final answer, using a hybrid of rule-based checks for verifiable problems and an LVM-based judge for nuanced tasks. The second is a Think Reward that holistically evaluates the entire reasoning path on its logical soundness, factual grounding, and answer consistency. The third is a Format Reward to ensure adherence to a specific output structure, with the thinking process to be in <think> and </think> tags and the final answer in a  $ \boxed{} $  tag. All reward signals are discrete binary values (0 or 1), and they are integrated into a final reward formulation where the coefficients of the three components sum to 1.

Training Recipe. We follow the same training recipe as the RL with Verifiable Rewards stage described previously. The Think-Fusion SFT model is optimized using PPO-based algorithms (DAPO for dense models, GSPO for MoE models) on the curated 150K sample dataset. This optimization process is carried out on our curated dataset of 150K high-quality samples. To maintain a controlled experimental setup, all training hyperparameters—such as learning rate, batch size, and optimizer settings—are kept identical to the prior stage. A key outcome we observe from this process is that the model effectively internalizes the reasoning capabilities it was taught. In other words, it learns the logical pathways to the correct answer without needing to explicitly write out each step. This allows it to provide accurate direct answers even when a user does not specifically prompt it for a step-by-step thinking process, making the final model both powerful and efficient in practice.

## 5 Training Infrastructure

To ensure efficiency and robustness during training, we design a systematic infrastructure framework tailored for large-scale multimodal models.

### 5.1 Stream Packing

To enhance training efficiency and reduce computational waste, we design a systematic stream packing strategy that jointly optimizes the handling of language and vision tokens. This strategy integrates two complementary components: batching with online packing and visual packing, ensuring balanced workloads, reduced memory footprint, and even improved model performance.

Batching and Online Packing. In conventional LVM training, all sequences in a batch are padded to the length of the longest sequence, resulting in severe inefficiency when sample lengths are highly skewed. We address this by concatenating variable-length samples into continuous streams, thereby minimizing the number of padding tokens. To preserve independence across samples, we adjust positional embeddings and attention masks accordingly. To further improve efficiency, each node maintains a buffer of candidate samples, from which micro-batches are dynamically constructed. This online packing procedure enhances sample diversity, maximizes GPU utilization by filling sequences to the hardware-supported maximum length, and achieves balanced workloads across devices. Moreover, to prevent starvation of long samples that exceed the maximum sequence length, we enforce their periodic inclusion during batch construction.

Visual Packing. Beyond balancing text token lengths, effective LVM training must also address discrepancies in visual token counts. In practice, different samples may produce substantially different numbers of visual tokens, particularly in models such as SAIL-VL2-AnyRes, where original image resolutions are preserved, leading to workload imbalance across devices and increased computational cost for the vision encoder. To mitigate this, we extend stream packing with a visual token balancing constraint. During sample selection from the buffer, we impose an additional condition that equalizes the number of visual tokens across devices. This ensures balanced computational loads for both the LLM and the vision encoder, thereby improving overall training efficiency.

Training Efficiency and Performance Analysis. Our packing strategies bring significant improvements in both training efficiency and model performance. Compared with the baseline, data packing nearly doubles Streaming Multiprocessor (SM) utilization and accelerates training by 50%, while visual packing alleviates excessive memory usage in the vision encoder, leading to a further 48% gain in efficiency. Beyond efficiency, ablation studies show that models trained with packing achieve an average +0.7% improvement over the baseline, with particularly strong gains on open-ended QA benchmarks such as LLaVA-Bench and MMVet, owing to the mitigation of sequence truncation and more effective training on long-context multimodal inputs.