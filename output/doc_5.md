#### 3.1.1 SAIL-Caption2

We upgrade SAIL-Caption2 in two aspects. First, we introduce a caption judge model that provides comprehensive quality scoring, based on which low-quality samples are filtered out to improve dataset reliability. Second, we broaden the diversity of captions by incorporating chart-related data, including both newly annotated chart captions and code-generated descriptions.

Automatic Caption Quality Assessment and Filtering. We assess caption quality from two perspectives: Visual Information Richness (VIR) and Image-Text Alignment (ITA).

• VIR evaluates the informativeness of visual content across three aspects: (a) Instances & Entities: number, diversity, and distinctiveness; (b) Visual Complexity: color variety, composition, and texture; (c) Fine-Grained Details: attributes such as textures, small objects, or contextual relationships.

- ITA measures how well captions match visual content across: (a) Specificity: precision of details; (b) Completeness: coverage of all key elements; (c) Accuracy: absence of hallucinations or redundancy.

For each evaluation dimension, captions are rated on a 1–5 scale, where higher scores indicate superior quality. We sample approximately 3M captions from SAIL-Caption and obtain scoring annotations using the powerful LVM API. Our analysis reveals that 15–20% of captions fall below an acceptable threshold (score < 3), highlighting substantial noise in the raw corpus. Directly relying on the LVM API to clean the entire dataset would incur prohibitive costs. Therefore, we train two judge models on SAIL-VL-2B: a Score Judge (1–5 rating) and a Yes-or-No Judge (binary quality decision). To support robust judge model training, we construct a balanced dataset by uniformly resampling  $ \sim $ 500K captions across the full score distribution. Both judge models achieve over 90% accuracy and recall (Table 2). Notably, the base model SAIL-VL-1.5 already outperforms comparable zero-shot baselines such as InternVL3, demonstrating the strength and reliability of our judge model foundation. We then clean the dataset by removing captions with scores < 3 or judged as “No.” Given judge accuracy above 90%, the retained captions are estimated to reach  $ >99\% $  high quality.


<table border=1 style='margin: auto; width: max-content;'><tr><td colspan="3">ITA Judge Model</td><td colspan="3">VIR Judge Model</td></tr><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>Precision</td><td style='text-align: center;'>Recall</td><td style='text-align: center;'>Model</td><td style='text-align: center;'>Precision</td><td style='text-align: center;'>Recall</td></tr><tr><td colspan="6">Fine-tuned</td></tr><tr><td style='text-align: center;'>SAIL-VL-1.5-2B-ITA-Score</td><td style='text-align: center;'>0.916</td><td style='text-align: center;'>0.903</td><td style='text-align: center;'>SAIL-VL-1.5-2B-VIR-Score</td><td style='text-align: center;'>0.983</td><td style='text-align: center;'>0.975</td></tr><tr><td style='text-align: center;'>SAIL-VL-1.5-2B-ITA-YorN</td><td style='text-align: center;'>0.906</td><td style='text-align: center;'>0.924</td><td style='text-align: center;'>SAIL-VL-1.5-2B-VIR-YorN</td><td style='text-align: center;'>0.934</td><td style='text-align: center;'>0.954</td></tr><tr><td style='text-align: center;'>SAIL-VL-1.5-8B-ITA-Score</td><td style='text-align: center;'>0.927</td><td style='text-align: center;'>0.899</td><td style='text-align: center;'>SAIL-VL-1.5-8B-VIR-Score</td><td style='text-align: center;'>0.982</td><td style='text-align: center;'>0.976</td></tr><tr><td style='text-align: center;'>SAIL-VL-1.5-8B-ITA-YorN</td><td style='text-align: center;'>0.916</td><td style='text-align: center;'>0.919</td><td style='text-align: center;'>SAIL-VL-1.5-8B-VIR-YorN</td><td style='text-align: center;'>0.928</td><td style='text-align: center;'>0.961</td></tr><tr><td colspan="6">Zero-shot</td></tr><tr><td style='text-align: center;'>InternVL3-2B</td><td style='text-align: center;'>0.269</td><td style='text-align: center;'>0.070</td><td style='text-align: center;'>InternVL3-2B</td><td style='text-align: center;'>0.430</td><td style='text-align: center;'>0.999</td></tr><tr><td style='text-align: center;'>SAIL-VL-1.5-2B</td><td style='text-align: center;'>0.436</td><td style='text-align: center;'>0.994</td><td style='text-align: center;'>SAIL-VL-1.5-2B</td><td style='text-align: center;'>0.823</td><td style='text-align: center;'>0.560</td></tr></table>

<div style="text-align: center;">Table 2: Evaluation of the effectiveness and reliability of our ITA and VIR judge models.</div>


By applying the proposed judge models and filtering criteria, we distill the original 300M captions into a corpus of 250M high-quality image-caption pairs.

Chart Caption Data. Beyond general image captions, we curate a large-scale chart caption corpus to improve the model's chart and table understanding, combining code-based generation with conventional datasets. We first design a chart data pipeline to automatically and continuously generate high-quality training data. Given a prompt, large language models (LLMs) produce chart code (SVG or Python), along with captions and QA pairs. The code is rendered into chart images, and the images, captions, and QA pairs are stored as standardized triplets for training. The pipeline supports a wide range of chart types, such as bar, line, pie, scatter, area, histogram, heatmap, and box plots, and allows flexible configuration of parameters, including language, model, chart type, and generation rate, while ensuring semantic validity. To complement this synthetic pipeline, we also collect open-source chart caption datasets in both English and Chinese, including PixMo-cap (Deitke et al., 2025) and DVQA (Kafle et al., 2018), to enhance diversity. Finally, we design an LLM-based annotation workflow for chart captioning and QA generation. Multimodal models annotate both open-source and auto-rendered charts under carefully designed prompts, with manual sampling for quality control. This process yields a large volume of reliable chart annotations. Through this integrated pipeline, we obtain a large corpus of high-quality chart