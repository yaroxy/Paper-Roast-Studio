
<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Stages</td><td style='text-align: center;'>LongCoT SFT</td><td style='text-align: center;'>Verifiable-Reward RL</td><td style='text-align: center;'>Think-Fusion SFT</td><td style='text-align: center;'>Mixed-Reward RL</td></tr><tr><td style='text-align: center;'>Data</td><td style='text-align: center;'>Multimodal CoT Data</td><td style='text-align: center;'>STEM RL data</td><td style='text-align: center;'>Multimodal CoT Data SAIL-Instructionv2</td><td style='text-align: center;'>STEM RL Data General RL Data</td></tr><tr><td style='text-align: center;'>Tokens</td><td style='text-align: center;'>0.4B</td><td style='text-align: center;'>0.1B</td><td style='text-align: center;'>1B</td><td style='text-align: center;'>0.2B</td></tr><tr><td style='text-align: center;'>Sequence length</td><td style='text-align: center;'>20480</td><td style='text-align: center;'>20480</td><td style='text-align: center;'>20480</td><td style='text-align: center;'>20480</td></tr><tr><td style='text-align: center;'>Trainable Params</td><td style='text-align: center;'>ALL</td><td style='text-align: center;'>ALL</td><td style='text-align: center;'>ALL</td><td style='text-align: center;'>ALL</td></tr></table>

<div style="text-align: center;">Table 4: Overview of advanced post-training stages: data composition, token volumes, sequence lengths, and trainable components stages.</div>



<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Model</td><td style='text-align: center;'>AVG</td><td style='text-align: center;'>ChartQA</td><td style='text-align: center;'>DocVQA</td><td style='text-align: center;'>InfoVQA</td><td style='text-align: center;'>MMBench $ _{\text{v1.1}} $</td><td style='text-align: center;'>MMMU</td><td style='text-align: center;'>MMStar</td><td style='text-align: center;'>OCRBench</td><td style='text-align: center;'>TextVQA</td><td style='text-align: center;'>RealWorldQA</td></tr><tr><td colspan="11">Homologous Model Merging</td></tr><tr><td style='text-align: center;'>Base model 1</td><td style='text-align: center;'>74.91</td><td style='text-align: center;'>83.88</td><td style='text-align: center;'>91.98</td><td style='text-align: center;'>74.03</td><td style='text-align: center;'>80.15</td><td style='text-align: center;'>45.89</td><td style='text-align: center;'>62.60</td><td style='text-align: center;'>87.00</td><td style='text-align: center;'>80.91</td><td style='text-align: center;'>67.71</td></tr><tr><td style='text-align: center;'>Base model 2</td><td style='text-align: center;'>74.54</td><td style='text-align: center;'>82.80</td><td style='text-align: center;'>91.24</td><td style='text-align: center;'>72.44</td><td style='text-align: center;'>79.33</td><td style='text-align: center;'>45.78</td><td style='text-align: center;'>61.27</td><td style='text-align: center;'>87.00</td><td style='text-align: center;'>81.06</td><td style='text-align: center;'>69.97</td></tr><tr><td style='text-align: center;'>Merge model</td><td style='text-align: center;'>76.60</td><td style='text-align: center;'>85.60</td><td style='text-align: center;'>92.22</td><td style='text-align: center;'>75.35</td><td style='text-align: center;'>81.46</td><td style='text-align: center;'>48.56</td><td style='text-align: center;'>63.33</td><td style='text-align: center;'>89.50</td><td style='text-align: center;'>82.05</td><td style='text-align: center;'>71.37</td></tr><tr><td colspan="11">Heterologous Model Merging</td></tr><tr><td style='text-align: center;'>Base model 1</td><td style='text-align: center;'>74.54</td><td style='text-align: center;'>82.80</td><td style='text-align: center;'>91.24</td><td style='text-align: center;'>72.44</td><td style='text-align: center;'>79.33</td><td style='text-align: center;'>45.78</td><td style='text-align: center;'>61.27</td><td style='text-align: center;'>87.00</td><td style='text-align: center;'>81.06</td><td style='text-align: center;'>69.97</td></tr><tr><td style='text-align: center;'>Base model 2</td><td style='text-align: center;'>75.41</td><td style='text-align: center;'>84.56</td><td style='text-align: center;'>92.30</td><td style='text-align: center;'>74.71</td><td style='text-align: center;'>80.19</td><td style='text-align: center;'>45.11</td><td style='text-align: center;'>63.27</td><td style='text-align: center;'>88.40</td><td style='text-align: center;'>81.66</td><td style='text-align: center;'>68.50</td></tr><tr><td style='text-align: center;'>Merge model</td><td style='text-align: center;'>12.86</td><td style='text-align: center;'>10.36</td><td style='text-align: center;'>5.25</td><td style='text-align: center;'>10.47</td><td style='text-align: center;'>12.73</td><td style='text-align: center;'>37.22</td><td style='text-align: center;'>30.73</td><td style='text-align: center;'>0.12</td><td style='text-align: center;'>8.83</td><td style='text-align: center;'>47.32</td></tr></table>

<div style="text-align: center;">Table 5: Performance of Homologous and Heterologous Model Merging on Various Datasets</div>


### 4.2 Post-Training Recipe

The post-training pipeline consists of three stages. The first stage focuses on foundational SFT to develop basic multimodal understanding, followed by a reasoning-oriented tuning pipeline. This pipeline includes a LongCoT SFT, a verifiable reward-driven RL, a Thinking-Fusion SFT, and a mixed reward-driven RL tuning phase.

#### 4.2.1 Basic Supervised Fine-Tuning

The Basic SFT stage is organized as a progressive knowledge injection pipeline. The first three phases focus on training the image modality, employing a curriculum learning strategy to gradually introduce increasingly complex data as training progresses, thereby fine-tuning all parameters of SAIL-VL2.

In the first phase, SAIL-VL2 learns foundational instruction-following abilities while incorporating world knowledge from the Infinity-MM Stage2 dataset (Gu et al., 2024). The second phase optimizes the model's visual instruction-following capabilities using SAIL-Instruction2, a newly curated 20M-sample high-quality visual instruction dataset. In the third phase, the model is exposed to a challenging subset of visual instruction data, including LLaVA-CoT, MMPR, and Condor datasets, enabling it to tackle a wider range of complex instruction-following tasks. To further enhance SAIL-VL2's understanding of video while preserving its proficiency in image comprehension, we introduce a video-image mixture training strategy in the final phase. This strategy combines high-quality image data from SAIL-Instruction2 with carefully filtered video data from the SAIL-Video dataset, maintaining a balanced 1:1 ratio between video and image data, thus ensuring effective multimodal learning.

Model Soup. To further enhance model performance after the SFT, we adopt a simple yet effective model soup strategy. We first categorize trained models into two groups: homogeneous models, which follow similar optimization trajectories such as comparable hyperparameter settings and data compositions, and heterogeneous models, which differ in these aspects. As shown in Table 5, experimental results reveal that merging homogeneous models consistently yields stable and notable performance improvements, whereas merging heterogeneous models often causes performance degradation. Based on these observations, our model soup strategy focuses on merging homogeneous models to achieve reliable performance gains.

#### 4.2.2 LongCoT Supervised Fine-tuning

The first stage of our post-training framework is LongCoT Supervised Fine-tuning, designed specifically to enhance the modelâ€™s step-by-step reasoning capabilities for complex problems. This is achieved