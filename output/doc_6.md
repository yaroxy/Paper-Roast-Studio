
<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Stages</td><td style='text-align: center;'>SAILViT Training</td><td style='text-align: center;'>Basic Pre-training</td><td style='text-align: center;'>Multi-task Pre-training</td><td style='text-align: center;'>Supervised Fine-tuning</td></tr><tr><td rowspan="3">Data</td><td style='text-align: center;'>SAIL-Captionv2</td><td style='text-align: center;'>SAIL-Captionv2 OCR</td><td style='text-align: center;'>SAIL-Captionv2</td><td style='text-align: center;'>SAIL-Instructionv2</td></tr><tr><td style='text-align: center;'>Captionv2QA</td><td style='text-align: center;'>Pareto text</td><td style='text-align: center;'>Captionv2QA</td><td style='text-align: center;'>SAIL-Video</td></tr><tr><td style='text-align: center;'>Multimodal VQA</td><td style='text-align: center;'>Multimodal VQA</td><td style='text-align: center;'>Multimodal VQA</td><td style='text-align: center;'>多模态VQA</td></tr><tr><td style='text-align: center;'>Tokens</td><td style='text-align: center;'>121B</td><td style='text-align: center;'>128B</td><td style='text-align: center;'>360B</td><td style='text-align: center;'>42B-&gt;40B-&gt;16B-&gt;70B</td></tr><tr><td style='text-align: center;'>Sequence length</td><td style='text-align: center;'>8192</td><td style='text-align: center;'>8192</td><td style='text-align: center;'>8192</td><td style='text-align: center;'>8192-&gt;8192-&gt;8192-&gt;16384</td></tr><tr><td style='text-align: center;'>Trainable Params</td><td style='text-align: center;'>Connector -&gt; Connector+ViT -&gt; ALL</td><td style='text-align: center;'>Connector</td><td style='text-align: center;'>ALL</td><td style='text-align: center;'>ALL-&gt;ALL-&gt;ALL-&gt;ALL</td></tr></table>

<div style="text-align: center;">Table 3: Overview of foundational pretraining training stages: data composition, token volumes, sequence lengths, and trainable components.</div>


data, consisting of 400K automatically generated captions and 1.29M captions collected from conventional chart datasets, with an equal distribution of English and Chinese.

Through the aforementioned filtering and collection strategies, we construct SAIL-Caption2, which consists of 250M general captions and 1.69M chart captions.

#### 3.1.2 Synthetic VQA Data

Beyond the original caption formulation, we further transform caption data into a QA format using LLMs, thereby scaling up QA data to support efficient and large-scale pre-training of SAIL-VL2. Specifically, we first sample approximately 80M entries from SAIL-Caption2 to construct synthetic VQA data. We then employ a powerful LLM API to convert each caption into multiple question-answer pairs. In this process, the model generates several diverse questions per caption to ensure broad coverage of the visual content, and subsequently produces corresponding answers based on the input text. Although synthetic data may introduce distributional biases in linguistic expression, as LLMs often produce homogenized phrasing with limited variability, we find that the model benefits substantially from such large-scale synthetic data: performance improves smoothly with increasing training budget, exhibiting a logarithmic scaling trend with up to 180M training samples.

### 3.2 Pre-Training Recipe

We next introduce the pre-training recipe, which consists of two progressive sub-stages designed to gradually activate SAIL-VL2's multimodal alignment and understanding. Beyond conventional scheduling strategies for LLM/LVM training, we introduce AdaLRS, a dynamic learning rate scheduler applied in the first stage to enable efficient optimization.

#### 3.2.1 Training Pipeline

The training pipeline of SAIL-VL2 consists of two progressive stages. The first stage performs basic multimodal pre-training, which equips the model with essential cross-modal alignment and understanding abilities, such as image captioning and OCR. The second stage extends this foundation through multi-task pre-training, enhancing the model's overall capacity for multimodal understanding and instruction following.

Basic Multimodal Pre-Training. In this stage, we develop SAIL-VL2's fundamental multimodal alignment and understanding abilities. Starting with a pre-trained SAIL-ViT, a language-pretrained LLM, we train a randomly initialized MLP-based adapter to bridge the gap between vision and language modalities. A total of 64M samples are used for model training in this stage, including general captions and chart captions from SAIL-Caption2, as well as high-quality OCR samples from IDL-WDS. Instead of training with a fixed learning rate, we set the initial learning rate as  $ 2 \times 10^{-4} $ , and apply the AdaLRS algorithm during training, which is elaborated in Section 3.2.3. AdaLRS adjusts the initial learning rate upwards to  $ 6.75 \times 10^{-4} $  effectively, surpassing the fixed learning rate baseline by a final loss advantage of over 0.06. The training batch size is set as 2048 in this stage.

Multi-task Pre-Training. Following the basic pre-training stage, we further conduct multi-task pre-training to comprehensively strengthen SAIL-VL2's visual understanding and instruction-following.