Experts (MoE) model, we use GSPO (Zheng et al., 2025), which provides stable and targeted updates to individual experts. The training is configured with a context length of 16384 and a maximum generation length of 4096 tokens. In each training episode, we generate 2048 rollouts from the policy and perform 8 gradient updates using a mini-batch size of 512. The learning rate for the policy network is set to  $ 1 \times 10^{-6} $ . To encourage exploration, the PPO clipping value is dynamically adjusted within the range of [0.20, 0.28].

#### 4.2.4 Think-Fusion Supervised Fine-tuning

Following the initial LongCoT SFT and RL with Verified Reward, the next stage of our post-training framework is Think-Fusion Supervised Fine-tuning. It is designed to simultaneously enhance the model's reasoning capabilities while maintaining its broad general understanding. This is achieved by fine-tuning the model on a mixed dataset strategically composed to address this dual objective.

Data Composition and Sampling. The dataset for this stage is constructed as a strategic mixture to balance two key objectives: maintaining the model's general capabilities while enhancing its specialized reasoning skills. The large majority, approximately 90% of the data, consists of general-purpose, direct-answer instruction pairs. This component covers a wide range of common tasks and serves to reinforce the model's wide-ranging abilities, ensuring it remains a capable and versatile assistant. The remaining 10% of the dataset is composed of high-quality Chain-of-Thought (CoT) exemplars. This smaller, focused portion is specifically chosen to cultivate the model's ability to perform complex, step-by-step reasoning. Crucially, these are not just any randomly selected CoT examples. Instead, they are carefully harvested from our preceding reinforcement learning (RL) stage through a process of rejection sampling. This means we selected only the best-performing reasoning chains that the model itself generated, ensuring the examples are both correct and logically sound. In the end, we get a final dataset for this stage comprising 100K high-quality CoT samples and 900K direct question-answering samples, creating a total of one million training instances.

Training Recipe. We fine-tune the model on the mixed dataset, and all model parameters are trained for one epoch using the AdamW optimizer with a global batch size of 1024. We employ a cosine learning rate schedule with a peak learning rate of  $ 1 \times 10^{-6} $ . The optimization objective is a standard next-token prediction, where the loss is applied conditionally to teach the model both response styles. For direct-answer pairs  $ (I, A) $ , where I is the instruction and A is the answer, the loss is computed only on the answer tokens. For CoT exemplars  $ (I, T, A) $ , where T is the thinking process, the loss is computed across the entire sequence, including both the thinking process and the final answer. This dual-target loss function can be formally denoted as:

 $$ \mathcal{L}_{\mathrm{Think-Fusion SFT}}=-\frac{1}{|D|}\left(\sum_{(I,T,A)\in D_{\mathrm{CoT}}}\log P_{\theta}(T\circ A|I)+\sum_{(I,A)\in D_{\mathrm{direct}}}\log P_{\theta}(A|I)\right) $$ 

where D is the entire mixed dataset, and  $ \circ $  denotes string concatenation. This training strategy explicitly teaches the model to adapt its output, providing concise answers for simple queries while generating detailed reasoning for complex problems that require it.

#### 4.2.5 Reinforcement Learning with a Mixed Reward System

Following the Think-Fusion Supervised Fine-tuning stage, we further enhance the model's reasoning capabilities through an RL stage. This stage is guided by a mixed reward system that provides comprehensive feedback beyond a single correctness score. Our reward system integrates three primary signals: the Answer Reward, which assesses final answer quality; the Thinking Reward, which evaluates the reasoning process; and a Format Reward, which encourages adherence to the desired output structure. Notably, for tasks where ground truth is not deterministically verifiable, we leverage an LVM-based judge to provide a nuanced reward signal. By optimizing the model with this comprehensive reward system on a diverse dataset, we enhance its performance on multimodal reasoning tasks, ensuring both high accuracy and logical coherence.

Data Curation. To further challenge the model, we refine the dataset from the preceding Reasoning RL stage. We apply a rejection sampling methodology to the previous set of reasoning-intensive samples, a process that filters for "hard cases" where the model's performance was suboptimal. This creates a more challenging, curated dataset of 50K stem samples. To ensure the model retains its broad conversational and instruction-following capabilities while improving on these hard cases, we combine these specialized samples with 50K new general-purpose samples from LLaVA-OneVision (Li et al., 2024a). The final training dataset, therefore, consists of 100K samples, balancing advanced reasoning with general utility.