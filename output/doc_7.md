capabilities. In this phase, all model parameters are unfrozen for joint optimization. Apart from general caption and OCR data as used in the basic pre-training stage, we integrate a series of instruction-tuning datasets for model training, including open-source VQA data, synthetic VQA data, as well as text-based math reasoning samples. These instruction tuning data not only enhance SAIL-VL2's visual instruction following abilities, but also preserve the LLM's language abilities and therefore enable large-scale vision-language alignment training with visual understanding data. In total, 180M samples with equal-sized visual understanding and instruction-tuning data are used in the multi-task pre-training stage. AdaLRS is not applied in this stage because the training loss on instruction tuning data exhibits poor correlation with model performance.

#### 3.2.2 Data Resampling

To improve training efficiency and data utilization in the pre-training stage of SAIL-VL2, we design a two-step resampling strategy. The goal is to mitigate distributional bias in large-scale caption and VQA data, enhance diversity, and prevent mode collapse, thereby enabling more effective multimodal alignment and instruction tuning.

In the first step, resampling is conducted at the dataset level during the basic pre-training stage. Different datasets naturally exhibit distinct distributional biases. By balancing the sampling ratios across datasets, we increase the diversity of imageâ€“text pairs and ensure that the adapter is exposed to heterogeneous visual signals, which supports efficient alignment of vision features into the language space.

In the second step, resampling is performed at the linguistic level during the multi-task pre-training stage. We focus on SAIL-Caption2 and Synthetic VQA datasets, which are large in scale but often show imbalanced language patterns because many samples are generated or post-processed by LLMs. To address this, we rebalance the data at the n-gram level, thereby improving lexical and structural diversity. This linguistic-level resampling boosts data efficiency, allowing SAIL-VL2 to achieve stronger multimodal understanding and more robust instruction-following capabilities.

#### 3.2.3 Adaptive Learning Rate Search

To achieve more efficient and effective optimization, we introduce an Adaptive Learning Rate Search (Dong et al., 2025b) (AdaLRS) strategy during the basic multimodal pre-training of SAIL-VL2. AdaLRS is motivated by an empirical observation that the training loss of LLM/LVM pre-training, as well as the loss descent velocity with respect to the learning rate, exhibits a convex pattern with a shared optimum. Inspired by this finding, AdaLRS dynamically adjusts the learning rate with a backtracking line search strategy based on the loss slope. Specifically: (1) when the loss descent slows down, the learning rate is tentatively increased; (2) if the increase accelerates loss reduction, the adjustment is retained and training proceeds; (3) if the increase further slows convergence, the model parameters and optimizer state are rolled back and the learning rate is decreased instead; (4) through this procedure, suboptimal learning rates can be efficiently adjusted toward the neighborhood of the optimal value within a limited number of steps. The AdaLRS algorithm can be formulated as:

 $$ \eta_{t+k}=\begin{cases}\alpha^{\prime}\eta_{t}&if v(\alpha^{\prime}\eta_{t})>v(\eta_{t})+2e\quad(loss slope increases\uparrow),\\\beta^{\prime}\eta_{t}&if v(\alpha^{\prime}\eta_{t})<v(\eta_{t})-2e\quad(loss slope decreases\downarrow),\\\eta_{t}&otherwise.\end{cases} $$ 

 $ v(\cdot) $  indicates the estimated loss curve slope obtained from a k-step window, while e stands for the estimation error between v and the true loss descent velocity V.  $ \alpha' $  and  $ \beta' $  are rectified LR scaling factors which satisfy  $ \alpha' = \max(\lambda^{t}\alpha, 1) $ ,  $ \beta' = \frac{1}{\max(\lambda^{t}\beta, 1)} $ , where  $ \alpha $ ,  $ \beta > 1 $  are co-prime integers and  $ \lambda \in (0, 1) $  is a decay factor.

#### 3.2.4 Scaling-law

We investigate the impact of scaling multi-task pre-training data on the performance of SAIL-VL2. To this end, we expand the training budget to 360B tokens and systematically evaluate the model across a wide range of benchmark subsets. As illustrated in Figure 4, performance exhibits consistent and monotonic improvements, yielding a smooth empirical scaling curve. The training corpus consists of general captions and VQA data, with approximately 50% of the latter synthesized by annotator models such as SAIL-Captioner (Dong et al., 2025a) and Qwen3 (Yang et al., 2025). While synthetic supervision may introduce linguistic biases, the increased scale and diversity of training data substantially enhance generalization and reasoning. These findings highlight data scaling as a critical factor driving performance gains in the multi-task pre-training stage, and emphasize the importance of large-scale, diverse corpora for advancing multimodal understanding.