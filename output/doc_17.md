
<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Benchmark</td><td style='text-align: center;'>SAIL-VL2 2B</td><td style='text-align: center;'>SAIL-VL2 Anyres-2B</td><td style='text-align: center;'>SAIL-VL1.5 2B</td><td style='text-align: center;'>Qwen2.5-VL 3B</td><td style='text-align: center;'>Ovis-U1 3B</td><td style='text-align: center;'>InternVL3.5 2B</td><td style='text-align: center;'>InternVL3 2B</td><td style='text-align: center;'>Ovis2 2B</td></tr><tr><td style='text-align: center;'>OpenCompassavg</td><td style='text-align: center;'>70.31</td><td style='text-align: center;'>68.56</td><td style='text-align: center;'>68.08</td><td style='text-align: center;'>65.36</td><td style='text-align: center;'>69.94</td><td style='text-align: center;'>66.64</td><td style='text-align: center;'>64.97</td><td style='text-align: center;'>65.49</td></tr><tr><td style='text-align: center;'>OpenSourceavg</td><td style='text-align: center;'>51.07</td><td style='text-align: center;'>50.15</td><td style='text-align: center;'>47.01</td><td style='text-align: center;'>48.40</td><td style='text-align: center;'>42.26</td><td style='text-align: center;'>49.49</td><td style='text-align: center;'>46.65</td><td style='text-align: center;'>46.53</td></tr><tr><td colspan="9">General</td></tr><tr><td style='text-align: center;'>MMBenchv1.1</td><td style='text-align: center;'>86.77</td><td style='text-align: center;'>86.48</td><td style='text-align: center;'>85.05</td><td style='text-align: center;'>82.40</td><td style='text-align: center;'>85.01</td><td style='text-align: center;'>83.10</td><td style='text-align: center;'>84.25</td><td style='text-align: center;'>81.79</td></tr><tr><td style='text-align: center;'>MME</td><td style='text-align: center;'>76.58</td><td style='text-align: center;'>73.36</td><td style='text-align: center;'>73.67</td><td style='text-align: center;'>77.46</td><td style='text-align: center;'>76.27</td><td style='text-align: center;'>75.99</td><td style='text-align: center;'>77.44</td><td style='text-align: center;'>72.32</td></tr><tr><td style='text-align: center;'>MMStar</td><td style='text-align: center;'>64.07</td><td style='text-align: center;'>63.40</td><td style='text-align: center;'>62.80</td><td style='text-align: center;'>55.13</td><td style='text-align: center;'>61.20</td><td style='text-align: center;'>57.2</td><td style='text-align: center;'>61.33</td><td style='text-align: center;'>58.13</td></tr><tr><td style='text-align: center;'>RealWorldQA</td><td style='text-align: center;'>72.29</td><td style='text-align: center;'>69.80</td><td style='text-align: center;'>67.06</td><td style='text-align: center;'>65.36</td><td style='text-align: center;'>70.07</td><td style='text-align: center;'>60.92</td><td style='text-align: center;'>64.58</td><td style='text-align: center;'>66.41</td></tr><tr><td style='text-align: center;'>AI2D</td><td style='text-align: center;'>83.00</td><td style='text-align: center;'>82.67</td><td style='text-align: center;'>83.68</td><td style='text-align: center;'>80.73</td><td style='text-align: center;'>85.88</td><td style='text-align: center;'>78.63</td><td style='text-align: center;'>78.72</td><td style='text-align: center;'>82.80</td></tr><tr><td style='text-align: center;'>DocVQA</td><td style='text-align: center;'>93.10</td><td style='text-align: center;'>92.86</td><td style='text-align: center;'>91.62</td><td style='text-align: center;'>93.11</td><td style='text-align: center;'>94.10</td><td style='text-align: center;'>88.49</td><td style='text-align: center;'>87.46</td><td style='text-align: center;'>91.49</td></tr><tr><td style='text-align: center;'>OCRBench</td><td style='text-align: center;'>89.50</td><td style='text-align: center;'>84.70</td><td style='text-align: center;'>88.50</td><td style='text-align: center;'>83.10</td><td style='text-align: center;'>88.10</td><td style='text-align: center;'>83.60</td><td style='text-align: center;'>83.40</td><td style='text-align: center;'>86.80</td></tr><tr><td style='text-align: center;'>MMVet</td><td style='text-align: center;'>68.67</td><td style='text-align: center;'>67.80</td><td style='text-align: center;'>62.75</td><td style='text-align: center;'>64.91</td><td style='text-align: center;'>69.08</td><td style='text-align: center;'>69.45</td><td style='text-align: center;'>66.19</td><td style='text-align: center;'>57.34</td></tr><tr><td style='text-align: center;'>HallusionBench</td><td style='text-align: center;'>51.74</td><td style='text-align: center;'>51.60</td><td style='text-align: center;'>49.80</td><td style='text-align: center;'>48.29</td><td style='text-align: center;'>55.42</td><td style='text-align: center;'>48.16</td><td style='text-align: center;'>41.42</td><td style='text-align: center;'>50.05</td></tr><tr><td style='text-align: center;'>CRPErelation</td><td style='text-align: center;'>75.20</td><td style='text-align: center;'>75.46</td><td style='text-align: center;'>73.92</td><td style='text-align: center;'>72.97</td><td style='text-align: center;'>75.15</td><td style='text-align: center;'>75.53</td><td style='text-align: center;'>71.58</td><td style='text-align: center;'>73.09</td></tr><tr><td style='text-align: center;'>MMMUval</td><td style='text-align: center;'>47.67</td><td style='text-align: center;'>45.00</td><td style='text-align: center;'>44.89</td><td style='text-align: center;'>48.11</td><td style='text-align: center;'>45.33</td><td style='text-align: center;'>51.11</td><td style='text-align: center;'>47.11</td><td style='text-align: center;'>42.67</td></tr><tr><td style='text-align: center;'>MM-IFEval</td><td style='text-align: center;'>52.21</td><td style='text-align: center;'>53.26</td><td style='text-align: center;'>34.27</td><td style='text-align: center;'>42.68</td><td style='text-align: center;'>46.62</td><td style='text-align: center;'>46.04</td><td style='text-align: center;'>33.95</td><td style='text-align: center;'>42.09</td></tr><tr><td style='text-align: center;'>Refcocoavg</td><td style='text-align: center;'>53.28</td><td style='text-align: center;'>57.82</td><td style='text-align: center;'>7.87</td><td style='text-align: center;'>34.15</td><td style='text-align: center;'>48.84</td><td style='text-align: center;'>40.35</td><td style='text-align: center;'>25.33</td><td style='text-align: center;'>31.91</td></tr><tr><td style='text-align: center;'>Overall</td><td style='text-align: center;'>70.31</td><td style='text-align: center;'>69.55</td><td style='text-align: center;'>63.53</td><td style='text-align: center;'>65.26</td><td style='text-align: center;'>69.31</td><td style='text-align: center;'>66.04</td><td style='text-align: center;'>63.29</td><td style='text-align: center;'>64.38</td></tr><tr><td colspan="9">Math &amp; Reasoning</td></tr><tr><td style='text-align: center;'>LogicVista</td><td style='text-align: center;'>36.24</td><td style='text-align: center;'>35.79</td><td style='text-align: center;'>35.35</td><td style='text-align: center;'>36.02</td><td style='text-align: center;'>33.33</td><td style='text-align: center;'>43.40</td><td style='text-align: center;'>33.56</td><td style='text-align: center;'>33.56</td></tr><tr><td style='text-align: center;'>MathVision</td><td style='text-align: center;'>23.36</td><td style='text-align: center;'>19.57</td><td style='text-align: center;'>17.60</td><td style='text-align: center;'>17.96</td><td style='text-align: center;'>19.24</td><td style='text-align: center;'>19.57</td><td style='text-align: center;'>20.23</td><td style='text-align: center;'>18.22</td></tr><tr><td style='text-align: center;'>MathVersemini</td><td style='text-align: center;'>31.19</td><td style='text-align: center;'>30.86</td><td style='text-align: center;'>29.87</td><td style='text-align: center;'>32.97</td><td style='text-align: center;'>37.61</td><td style='text-align: center;'>32.87</td><td style='text-align: center;'>32.69</td><td style='text-align: center;'>28.27</td></tr><tr><td style='text-align: center;'>MathVistamini</td><td style='text-align: center;'>71.10</td><td style='text-align: center;'>66.80</td><td style='text-align: center;'>67.20</td><td style='text-align: center;'>60.20</td><td style='text-align: center;'>69.50</td><td style='text-align: center;'>61.90</td><td style='text-align: center;'>57.30</td><td style='text-align: center;'>64.30</td></tr><tr><td style='text-align: center;'>OlympiadBenchmini</td><td style='text-align: center;'>7.54</td><td style='text-align: center;'>3.61</td><td style='text-align: center;'>2.30</td><td style='text-align: center;'>4.26</td><td style='text-align: center;'>5.25</td><td style='text-align: center;'>10.16</td><td style='text-align: center;'>5.57</td><td style='text-align: center;'>3.61</td></tr><tr><td style='text-align: center;'>WeMath</td><td style='text-align: center;'>22.67</td><td style='text-align: center;'>23.33</td><td style='text-align: center;'>17.14</td><td style='text-align: center;'>20.67</td><td style='text-align: center;'>16.76</td><td style='text-align: center;'>18.95</td><td style='text-align: center;'>12.95</td><td style='text-align: center;'>10.10</td></tr><tr><td style='text-align: center;'>DynaMath</td><td style='text-align: center;'>10.18</td><td style='text-align: center;'>10.58</td><td style='text-align: center;'>9.98</td><td style='text-align: center;'>10.98</td><td style='text-align: center;'>17.76</td><td style='text-align: center;'>15.17</td><td style='text-align: center;'>13.97</td><td style='text-align: center;'>10.78</td></tr><tr><td style='text-align: center;'>Overall</td><td style='text-align: center;'>28.90</td><td style='text-align: center;'>27.22</td><td style='text-align: center;'>25.63</td><td style='text-align: center;'>26.15</td><td style='text-align: center;'>28.49</td><td style='text-align: center;'>28.86</td><td style='text-align: center;'>25.18</td><td style='text-align: center;'>24.12</td></tr><tr><td colspan="9">Multi-images &amp; Video</td></tr><tr><td style='text-align: center;'>Video-MMEw/o sub</td><td style='text-align: center;'>57.10</td><td style='text-align: center;'>55.30</td><td style='text-align: center;'>56.60</td><td style='text-align: center;'>60.60</td><td style='text-align: center;'>20.80</td><td style='text-align: center;'>55.00</td><td style='text-align: center;'>55.60</td><td style='text-align: center;'>54.70</td></tr><tr><td style='text-align: center;'>LongVideoBenchval</td><td style='text-align: center;'>54.45</td><td style='text-align: center;'>53.63</td><td style='text-align: center;'>53.18</td><td style='text-align: center;'>54.23</td><td style='text-align: center;'>24.31</td><td style='text-align: center;'>51.53</td><td style='text-align: center;'>50.11</td><td style='text-align: center;'>50.79</td></tr><tr><td style='text-align: center;'>TempCompassavg</td><td style='text-align: center;'>61.86</td><td style='text-align: center;'>62.79</td><td style='text-align: center;'>61.53</td><td style='text-align: center;'>62.48</td><td style='text-align: center;'>35.57</td><td style='text-align: center;'>63.20</td><td style='text-align: center;'>62.33</td><td style='text-align: center;'>63.47</td></tr><tr><td style='text-align: center;'>MMIU</td><td style='text-align: center;'>42.61</td><td style='text-align: center;'>43.02</td><td style='text-align: center;'>36.18</td><td style='text-align: center;'>37.86</td><td style='text-align: center;'>35.19</td><td style='text-align: center;'>44.51</td><td style='text-align: center;'>37.88</td><td style='text-align: center;'>35.45</td></tr><tr><td style='text-align: center;'>Overall</td><td style='text-align: center;'>54.01</td><td style='text-align: center;'>53.69</td><td style='text-align: center;'>51.87</td><td style='text-align: center;'>53.79</td><td style='text-align: center;'>28.97</td><td style='text-align: center;'>53.56</td><td style='text-align: center;'>51.48</td><td style='text-align: center;'>51.10</td></tr></table>

<div style="text-align: center;">Table 8: Overall comparison of the SAIL-VL2 series and existing open-source MLLMs (<4B). Open-Compass incorporates eight evaluation datasets: MMBench v1.1, MMStar, AI2D, OCRBench, MMVet, HallusionBench, MMMUval, and MathVistamini. The OpenSource metric is computed as the average score across three dimensions: General, Math & Reasoning, and Multi-image & Video. Refcocoavg is obtained by averaging five test sets: refcocotestA, refcocotestB, refcocotest, refcocoplustestA, and refcocoplustestB. OlympiadBench $ _{mini} $  denotes a randomly sampled subset of the original evaluation set. For the Refcoco series, prompts explicitly specified output coordinates in the 0–1000 range, with safeguards to handle outputs in the 0–1 range. Video results are reported using 16 randomly sampled frames as visual input.</div>


#### 6.2.2 Multimodal Understanding Tasks

General Multimodal Understanding. To comprehensively evaluate the general multimodal understanding capabilities of SAIL-VL2 across multiple dimensions, including visual question answering, document understanding, and OCR, we conduct extensive experiments on a diverse range of benchmarks.

As reported in Table 8 and 9, SAIL-VL2-2B and SAIL-VL2-8B achieve state-of-the-art performance across diverse benchmarks covering multiple evaluation dimensions. At comparable parameter scales, both models consistently outperform prior open-source counterparts of the same size (2B/8B) on representative datasets, including the MMBench series (Liu et al., 2024a), MMStar (Liu et al., 2024a), RealWorldQA (X, 2025), DocVQA (Mathew et al., 2021), and OCRBench (Liu et al., 2024b). These results highlight the superior general multimodal understanding capabilities of SAIL-VL2.

In the domain of general visual question answering, SAIL-VL2-2B achieves leading performance, obtaining 86.77 on MMBench-v1.1, 72.29 on RealWorldQA, and 64.07 on MMStar. These results surpass previous sub-4B state-of-the-art LVMs, including Ovis-U1, Qwen2.5-VL-3B, and InternVL3.5-2B, establishing SAIL-VL2-2B as a benchmark efficient model for visual detail understanding and demonstrating the feasibility of achieving high performance with compact architectures.

In the domain of document image understanding, SAIL-VL2 achieves consistently strong results across OCR and document comprehension tasks. On OCRBench, SAIL-VL2-2B attains a score of 895, setting