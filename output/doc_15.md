
<table border=1 style='margin: auto; width: max-content;'><tr><td style='text-align: center;'>Settings</td><td style='text-align: center;'>AIMv2-Large</td><td style='text-align: center;'>InternViT-300M-448px-V2.5</td><td style='text-align: center;'>SAIL-ViT-Large</td><td style='text-align: center;'>AIMv2-Huge</td><td style='text-align: center;'>SAIL-ViT-Huge</td><td style='text-align: center;'>SAIL-ViT-Huge-v2</td><td style='text-align: center;'>InternViT-6B-448px-V2.5</td></tr><tr><td style='text-align: center;'>ImageNet-1k</td><td style='text-align: center;'>79.88</td><td style='text-align: center;'>73.70</td><td style='text-align: center;'>80.71</td><td style='text-align: center;'>81.68</td><td style='text-align: center;'>82.21</td><td style='text-align: center;'>82.34</td><td style='text-align: center;'>84.18</td></tr><tr><td style='text-align: center;'>ImageNet-A</td><td style='text-align: center;'>25.41</td><td style='text-align: center;'>13.45</td><td style='text-align: center;'>29.31</td><td style='text-align: center;'>28.96</td><td style='text-align: center;'>33.04</td><td style='text-align: center;'>35.63</td><td style='text-align: center;'>46.27</td></tr><tr><td style='text-align: center;'>ImageNet-R</td><td style='text-align: center;'>55.73</td><td style='text-align: center;'>39.99</td><td style='text-align: center;'>56.42</td><td style='text-align: center;'>58.13</td><td style='text-align: center;'>60.33</td><td style='text-align: center;'>59.90</td><td style='text-align: center;'>59.88</td></tr><tr><td style='text-align: center;'>ImageNet-V2</td><td style='text-align: center;'>76.45</td><td style='text-align: center;'>69.55</td><td style='text-align: center;'>77.06</td><td style='text-align: center;'>77.32</td><td style='text-align: center;'>78.94</td><td style='text-align: center;'>79.12</td><td style='text-align: center;'>80.92</td></tr><tr><td style='text-align: center;'>Average</td><td style='text-align: center;'>59.37</td><td style='text-align: center;'>49.17</td><td style='text-align: center;'>60.87</td><td style='text-align: center;'>61.52</td><td style='text-align: center;'>63.63</td><td style='text-align: center;'>64.25</td><td style='text-align: center;'>67.81</td></tr></table>

<div style="text-align: center;">Table 6: Comparison results of different visual backbones on visual recognition tasks.</div>


and Mathematics," "Comprehensive Multimodal Assessment," "Visual Localization," "Multimodal Hallucination Assessment," and "Instruction Following."

â€¢ The OpenCompass dimension includes 8 datasets, aligned with the official OpenCompass leaderboard, covering tasks from multimodal understanding to reasoning.

- The open-source video understanding benchmarks consist of 9 datasets, incorporating all 5 video evaluation sets from the official OpenCompass video leaderboard, as well as four additional benchmarks: ActivityNetQA, LongvideoBench, NextQA, and TVBench.

Based on the comprehensive benchmarks mentioned above, we conduct a thorough performance evaluation and comparison with current methods, providing a holistic analysis of SAIL-VL2's capabilities.

Evaluation Protocol. For the comparison of SAIL-VL2 basic models, we adopt a unified evaluation framework based on a customized version of VLMEvalKit (Contributors, 2023). For datasets requiring LLM-assisted assessment, we employ Doubao-1.5-vision-pro-32k-250115 (Guo et al., 2025) as the evaluation API, and all baseline models are re-evaluated under this setting. All evaluation results for the SAIL-VL2-Thinking models are sourced from the official OpenCompass open-source leaderboard, with the exception of SAIL-VL2-A3B-Thinking and Keye-VL-8B-Thinking. To ensure a fair comparison for these two models, we conducted the evaluation using a customized VLMEvalKit with GPT-4O-Mini as the judge model. This process strictly aligns with the official OpenCompass settings, making our results directly comparable to the leaderboard.

### 6.2 Main Results

Based on the aforementioned settings, we conduct a comprehensive performance evaluation of SAIL-VL2 across these benchmarks. First, we analyze the performance of SAIL-ViT in detail. We then evaluate all basic (non-thinking) models in the SAIL-VL2 series using this extensive benchmark, presenting the results across several widely used evaluation datasets in Table 8 and Table 9. The 'thinking' models of SAIL-VL2 were assessed and compared exclusively on the official OpenCompass Reasoning leaderboard, as shown in Table 10. Experimental results show that SAIL-VL2-2B/8B and SAIL-VL2-Thinking-8B achieve state-of-the-art average performance on the OpenCompass evaluation, consistently surpassing all publicly available open-source models at comparable parameter scales.

#### 6.2.1 Experimental Analysis on SAIL-ViT

We verify the effectiveness of SAIL-ViT through two complementary evaluations: (1) its core visual classification performance, and (2) its capability to align visual features with multimodal representations.

Zero-shot Image Classification of SAIL-ViT. To evaluate the multimodal representation capabilities of SAIL-ViT, we conducted a comparative analysis with open-source ViT models of similar scale, focusing on the purely visual task of image classification. Specifically, we averaged the token outputs from the final block of various visual backbones, which were then processed through a two-layer MLP to generate a token representing the probability value. All models were trained for 10 epochs using the ImageNet-1k training dataset, followed by evaluation on four test datasets: ImageNet-1k (Russakovsky et al., 2015), ImageNet-A (Hendrycks et al., 2019), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-v2 (Recht et al., 2019). The experimental results are presented in Table 6. Under consistent experimental settings, SAIL-ViT-Large demonstrated significant improvements over the baseline model AIMv2 (Fini et al., 2025) on a CV benchmark consisting of four test sets, with an average gain of +1.5%. For SAIL-ViT-Huge, the average improvement was +2.11%. Additionally, by increasing the data scale during the world knowledge infusion stage, we developed SAIL-ViT, which further enhanced vision task performance by +2.73% compared to the baseline. Overall, these results highlight the superior performance and scalability of SAIL-ViT compared to baseline ViT models on image classification benchmarks.

Exploring Affinity among Multimodal Features. The core capability of the vision encoder is to align visual features with the LLM space. To evaluate this, we explore the affinity between multimodal features.